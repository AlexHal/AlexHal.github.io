[
  {
    "name": "Simple-Version-Control",
    "html_url": "https://github.com/AlexHal/Simple-Version-Control",
    "readme": "A lightweight C++ application for managing file versions using a linked list-based version control system. This tool allows users to track, compare, and restore previous versions of a file while maintaining simplicity.\n\n## Features\n\n- **Version Control**: \n  - Add file content as new versions only if changes are detected.\n  - Remove specific versions and renumber remaining versions.\n  - Restore any version to the original file.\n- **Comparison**: Compare two versions line-by-line, highlighting differences.\n- **Search**: Search all versions for a specific keyword.\n- **Persistence**: Save and load version data from a file for session continuity.\n\n## How It Works\n\n- Each file version is stored in a linked list, with unique hashes for change detection.\n- Users interact through a simple menu for adding, removing, printing, comparing, and searching versions.\n- File data and metadata are saved to a persistent storage file.\n\n## How to Run\n\n1. Compile the program:\n   <pre>  g++ -o file_versioning SVC.cpp </pre>\n"
  },
  {
    "name": "Thread-Scheduler",
    "html_url": "https://github.com/AlexHal/Thread-Scheduler",
    "readme": "A lightweight user-level thread scheduler implemented in C, designed to manage compute and I/O tasks with first-come, first-served (FCFS) scheduling.\n\n## Features\n\n- **User-Level Threads**: Implements thread creation, context switching, and termination.\n- **Dual Executors**: \n  - Compute Executor (C-EXEC): Handles compute tasks.\n  - I/O Executor (I-EXEC): Manages blocking I/O tasks to prevent delays in compute operations.\n- **Task Management**:\n  - FCFS scheduling using ready and wait queues.\n  - Tasks yield or terminate via `sut_yield()` and `sut_exit()`.\n  - Dynamic task creation with `sut_create()`.\n- **File I/O**: Support for opening, reading, writing, and closing files using dedicated functions (`sut_open()`, `sut_read()`, etc.).\n- **Graceful Shutdown**: Terminates executors and cleans up resources with `sut_shutdown()`.\n\n## API\n\n- `sut_init()`: Initializes the scheduler.\n- `sut_create(task_function)`: Creates a new task.\n- `sut_yield()`: Pauses the current task and schedules the next one.\n- `sut_exit()`: Terminates the current task.\n- File operations: `sut_open()`, `sut_read()`, `sut_write()`, `sut_close()`.\n- `sut_shutdown()`: Shuts down the scheduler and its executors.\n\n## Technologies\n\n- **Language**: C\n- **Platform**: Linux/Unix\n- **Key Libraries**: `makecontext()`, `swapcontext()`, queue management.\n\n## How to Run\n\n1. Clone this repository.\n2. Compile the library and main program:\n   <pre>  gcc -o thread_scheduler thread_scheduler.c </pre> \n"
  },
  {
    "name": "Shell",
    "html_url": "https://github.com/AlexHal/Shell",
    "readme": "A custom Unix shell implemented in C, showcasing core functionalities of a command-line interface and process management.\n\n## Features\n\n- **Command Execution**: Executes user commands using `fork()` and `execvp()` with support for foreground and background processes.\n- **Built-In Commands**: Includes:\n  - `cd`: Change directories.\n  - `pwd`: Print current directory.\n  - `echo`: Display messages.\n  - `exit`: Terminate the shell.\n  - `jobs`: List background processes.\n  - `fg`: Bring background jobs to the foreground.\n- **Redirection**: Redirects command output to files using `dup()`.\n- **Piping**: Implements command chaining with pipes via `pipe()`.\n\n## Technologies\n\n- **Language**: C\n- **Platform**: Linux/Unix\n- **Key System Calls**: `fork()`, `execvp()`, `pipe()`, `dup()`, `waitpid()`\n\n## How to Run\n\n1. Clone this repository.\n2. Compile the shell using `gcc`:\n   <pre>  gcc -o my_shell shell.c </pre>\n"
  },
  {
    "name": "AlexHal.github.io",
    "html_url": "https://github.com/AlexHal/AlexHal.github.io",
    "readme": "A personal portfolio website hosted on **GitHub Pages**, showcasing my programming projects, skills, and achievements. Built with **Jekyll**, the **Minimal theme**, and **GitHub Actions** for dynamic updates.\n\n## Features\n- **Dynamic Project Previews**: Automatically pulls project details (e.g., README files) from repositories.\n- **Responsive Design**: Optimized for desktop and mobile.\n- **Custom Styling**: Includes blurred text previews and gradient effects.\n- **Google Analytics Integration**: Tracks page views, user interactions, and traffic sources.\n- **Automated Deployment**: Updates and redeploys the site with GitHub Actions.\n\n## Technologies Used\n- **GitHub Pages**: Static hosting.\n- **Jekyll**: Static site generation.\n- **Minimal Theme**: Lightweight, responsive design.\n- **GitHub Actions**: Automates project updates and deployment.\n- **HTML, CSS, and Liquid**: Custom styles and templating.\n- **Google Analytics**: User tracking and site performance monitoring."
  },
  {
    "name": "Reproducibility-DNDT",
    "html_url": "https://github.com/AlexHal/Reproducibility-DNDT",
    "readme": "This repository contains a mini-project focused on reproducing the results of the Deep Neural Decision Trees (DNDT) model, as outlined in the 2018 paper *Deep Neural Decision Trees*. The project evaluates the claims of interpretability, performance, and simplicity of DNDT compared to traditional Decision Trees (DT) and Neural Networks (NN).\r\n\r\n\r\n## Overview\r\nThe goal of this project is to reproduce the DNDT model's results and evaluate its performance on multiple tabular datasets. The DNDT model combines the interpretability of Decision Trees with the power of Neural Networks, using gradient descent for training and GPU acceleration for scalability.\r\n\r\n\r\n## Datasets\r\n- **Iris Dataset**\r\n- **Titanic Dataset**\r\n- **Pima Indian Diabetes Dataset**\r\n- **Breast Cancer Wisconsin Dataset**\r\n\r\nEach dataset was preprocessed and analyzed to ensure compatibility with the DNDT model.\r\n\r\n## Methodology\r\n1. **Model Implementation**: DNDT, DT, and NN were implemented using PyTorch.\r\n2. **Reproducibility Scope**: Focused on claims regarding DNDT\u2019s performance and interpretability.\r\n3. **Experimental Setup**: Compared normalized vs. unnormalized data, hyperparameter tuning, and GPU acceleration.\r\n\r\n\r\n\r\n## Results\r\n- DNDT showed competitive accuracy but was dataset-dependent.\r\n- Interpretability through tree visualization was successfully demonstrated.\r\n- Challenges arose from incomplete details in the original paper (e.g., missing hyperparameters).\r\n\r\n\r\n## References\r\n- [Deep Neural Decision Trees Paper](https://arxiv.org/pdf/1806.06988v1.pdf)\r\n"
  },
  {
    "name": "Classification-of-Image-Data",
    "html_url": "https://github.com/AlexHal/Classification-of-Image-Data",
    "readme": "This project explores image classification using **Multilayer Perceptrons (MLPs)** and **Convolutional Neural Networks (CNNs)**. The goal is to analyze how hyperparameters, regularization, and optimization affect model performance on image datasets.\r\n\r\n## Key Highlights\r\n- Implemented MLPs with different weight initialization methods, hidden layers, and activation functions.\r\n- Built CNNs to compare performance with MLPs.\r\n- Benchmarked models on Fashion MNIST and CIFAR-10 datasets.\r\n\r\n## Technologies Used\r\n- **PyTorch**: Model implementation and training.\r\n- **Python**: Preprocessing and result visualization.\r\n\r\n## Datasets\r\n- **Fashion MNIST**: Zalando\u2019s article images (28x28 grayscale images).\r\n- **CIFAR-10**: Color images in 10 classes (32x32 pixels).\r\n\r\n## Results\r\n- **MLPs**:\r\n  - Xavier and Kaiming initialization yielded superior results.\r\n  - Regularization (L2) effectively reduced overfitting.\r\n- **CNNs**:\r\n  - Slightly outperformed MLPs, especially for CIFAR-10.\r\n  - Leveraged spatial relationships for improved feature extraction.\r\n\r\n## References\r\n- [Fashion MNIST Dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html)\r\n- [CIFAR-10 Dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html)\r\n"
  },
  {
    "name": "Classification-of-Textual-Data",
    "html_url": "https://github.com/AlexHal/Classification-of-Textual-Data",
    "readme": "This project implements **Naive Bayes** and **BERT** models for emotion classification on text data. It compares traditional and modern NLP techniques for detecting emotions such as anger, joy, and sadness.\r\n\r\nAdditionally, we used **BertViz** to visualize attention mechanisms in BERT, providing insights into how the model processes text for emotion detection.\r\n\r\n## Key Highlights\r\n- Built a Naive Bayes model from scratch using the bag-of-words approach.\r\n- Fine-tuned a pre-trained BERT model for emotion classification.\r\n- Visualized attention layers in BERT using BertViz.\r\n- Compared performance in terms of accuracy and F1 score.\r\n\r\n## Technologies Used\r\n- **Hugging Face Transformers**: Pre-trained BERT model.\r\n- **BertViz**: Visualized BERT's attention layers.\r\n- **PyTorch**: Fine-tuning and evaluation.\r\n- **Python**: Implemented Naive Bayes model and preprocessing.\r\n\r\n## Dataset\r\n- **Emotion Dataset**: English Twitter messages with six basic emotions (anger, fear, joy, love, sadness, and surprise).\r\n\r\n## Results\r\n- **Naive Bayes**:\r\n  - Achieved moderate accuracy but struggled with linguistic context.\r\n- **BERT**:\r\n  - Demonstrated superior performance due to contextual understanding.\r\n  - Pretraining significantly improved accuracy and F1 scores.\r\n- **BertViz**:\r\n  - Revealed how BERT attends to specific words, showcasing its contextual understanding.\r\n\r\n## Visualizations\r\n- **Attention Layers**: BertViz visualizations highlight how BERT processes and attends to key parts of input text during classification.\r\n\r\n"
  }
]